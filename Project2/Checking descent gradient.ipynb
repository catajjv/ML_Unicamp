{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project 2. \n",
    "#### Checking partial derivatives of descent gradient\n",
    "\n",
    "#### Catalina Jaramillo\n",
    "#### AngÃ©lica Narvaes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoid function\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derivative of sigmoid function\n",
    "\n",
    "def sigmoidGradient(z):\n",
    "    sigmoid = 1/(1+np.exp(-z))\n",
    "    return sigmoid*(1-sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# theta as a list\n",
    "\n",
    "def listTheta(thetai, input_layer_size, hidden_layer_size, num_labels):\n",
    "    \n",
    "    theta = []\n",
    "    \n",
    "    aux = 0\n",
    "     \n",
    "    # if there is only 1 hidden layer\n",
    "    if type(hidden_layer_size)==int:\n",
    "        theta.append(thetai[:((input_layer_size+1)*hidden_layer_size)].reshape(hidden_layer_size, input_layer_size+1))\n",
    "        theta.append(thetai[((input_layer_size+1)*hidden_layer_size):].reshape(num_labels, hidden_layer_size+1))\n",
    "        return theta\n",
    "    else:   \n",
    "        l=len(hidden_layer_size)\n",
    "        \n",
    "    # if there is more than 1 hidden layer    \n",
    "    for i in range(l+1):\n",
    "        if i==0:\n",
    "            aux = (input_layer_size+1)*hidden_layer_size[i]\n",
    "            theta.append(thetai[:aux].reshape(hidden_layer_size[i], input_layer_size+1))\n",
    "        elif i==l:\n",
    "            aux1= copy.deepcopy(aux)\n",
    "            aux = aux + (hidden_layer_size[i-1]+1)*num_labels\n",
    "            theta.append(thetai[aux1:aux].reshape(num_labels, hidden_layer_size[i-1]+1))\n",
    "        elif i>0 and i<l:\n",
    "            aux1= copy.deepcopy(aux)\n",
    "            aux = aux + (hidden_layer_size[i-1]+1)*(hidden_layer_size[i])\n",
    "            theta.append(thetai[aux1:aux].reshape(hidden_layer_size[i], hidden_layer_size[i-1]+1))\n",
    "\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# theta list as a flatten vector\n",
    "\n",
    "def flattenTheta(thetai, input_layer_size, hidden_layer_size, num_labels):\n",
    "    for i in range(len(thetai)):\n",
    "        if i==0:\n",
    "            theta = thetai[i].flatten()\n",
    "        else:\n",
    "            theta = np.insert(theta, len(theta), thetai[i].flatten())\n",
    "            \n",
    "    \n",
    "    return theta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of elements of layer size (camada de entrada)\n",
    "# number of elements of hidden layer\n",
    "# number of elements of output: num_labels\n",
    "\n",
    "def computeCost(thetai,X,y,input_layer_size, hidden_layer_size, num_labels, Lambda):\n",
    "                   \n",
    "    theta = listTheta(thetai, input_layer_size, hidden_layer_size, num_labels)\n",
    "    l=len(theta)\n",
    "    \n",
    "    m = X.shape[0]\n",
    "    # number of layers\n",
    "    \n",
    "    J = 0\n",
    "    \n",
    "    #join the vector of 1s\n",
    "    \n",
    "    X = np.hstack((np.ones([m,1]),X))\n",
    "    y10 = np.zeros([m,num_labels])\n",
    "    \n",
    "    #Activation list\n",
    "    a=[]\n",
    "\n",
    "    for i in range(l):\n",
    "        #1st activation\n",
    "        if i==0:\n",
    "            a1 = sigmoid(X @ theta[i].T)\n",
    "            a1 = np.hstack((np.ones([m,1]),a1))\n",
    "            a.append(a1)\n",
    "       \n",
    "        #final activation\n",
    "        elif i==l-1:\n",
    "            a2 = sigmoid(a[i-1] @ theta[i].T)\n",
    "            a.append(a2)\n",
    "            \n",
    "        #hidden activation\n",
    "        else:\n",
    "            a1 = sigmoid(a[i-1] @ theta[i].T)\n",
    "            a1 = np.hstack((np.ones([m,1]),a1))\n",
    "            a.append(a1)\n",
    "\n",
    "                \n",
    "    for i in range(1,num_labels+1):\n",
    "        #creates the matrix of ys, where each column is the vector of label, with 1 in position i if class=i\n",
    "    \n",
    "        ''' np.newaxis might come in handy when you want to explicitly convert a 1D array to \n",
    "            either a row vector or a column vector.\n",
    "        '''\n",
    "        y10[:,i-1][:,np.newaxis] = np.where(y==i,1,0)\n",
    "        \n",
    "    for j in range(num_labels):\n",
    "        #adding all log cost function\n",
    "        J = J - np.sum(y10[:,j]*np.log(a[l-1][:,j])+(1-y10[:,j])*np.log(1-a[l-1][:,j]))\n",
    "        \n",
    "    cost = (1/m)*J\n",
    "    \n",
    "    #regularized cost for logistic regression\n",
    "    regj = 0\n",
    "    # derivatives for each layer  \n",
    "    grad = []\n",
    "    for i in range(l):\n",
    "        regj = regj+np.sum(theta[i][:,1:]**2)   ####CHECK IF IT CAN BE CHANGED TO DOT PRODUCT.\n",
    "        grad.append(np.zeros((theta[i].shape)))\n",
    "      \n",
    "    reg_J = cost + Lambda/(2*m)*(regj)\n",
    "            \n",
    "    #computing update of thetas\n",
    "    for i in range(m):\n",
    "        xi = X[i,:]\n",
    "        \n",
    "        #activation i\n",
    "        ai= []\n",
    "        ai.append(xi)\n",
    "        \n",
    "        for j in range(l):\n",
    "            ai.append(a[j][i,:])\n",
    "       \n",
    "        # deltas \n",
    "        delta = []\n",
    "        #len(ai)=l+1\n",
    "        for j in range(l,0,-1):\n",
    "            # delta in output\n",
    "            if j==l:\n",
    "                # 1st delta\n",
    "                delta.append(ai[j]-y10[i,:])\n",
    "            \n",
    "            #delta hidden layer\n",
    "            \n",
    "            #the case of layer L-1 is different because of bias on L-1 and not bias on output\n",
    "            elif j==l-1:\n",
    "                delta.append((theta[j].T @ delta[l-j-1])*(ai[j])*(1-ai[j]))\n",
    "            \n",
    "            else: \n",
    "                #never delta(0)\n",
    "                delta.append((theta[j].T @ delta[l-j-1][1:])*(ai[j])*(1-ai[j]))\n",
    "        \n",
    "        # reverse delta because it's created by backpropagation\n",
    "        delta.reverse()\n",
    "     \n",
    "        # grad = grad anterior(layer l) + delta(layer l+1)*(a(layer l))^T \n",
    "        \n",
    "        #update grad\n",
    "        \n",
    "        for j in range(l):\n",
    "            # recall ai[j] = a[j-1]... len(ai)=l+1 because a1[0]=xi\n",
    "            if j==l-1:\n",
    "                grad[j] = grad[j] +  delta[j].T[:,np.newaxis]@ai[j][:,np.newaxis].T\n",
    "            else:\n",
    "                grad[j] = grad[j] + delta[j][1:][:,np.newaxis]@ ai[j][:,np.newaxis].T\n",
    "\n",
    "    grad_reg = []\n",
    "\n",
    "    for i in range(l):\n",
    "        grad[i] = (1/m)*grad[i]\n",
    "        grad_reg.append(grad[i]+(Lambda/m)*np.hstack((np.zeros([theta[i].shape[0],1]), theta[i][:,1:])))\n",
    "    \n",
    "\n",
    "    # flatten grad_red\n",
    "    for i in range (len(grad_reg)):\n",
    "        if i==0:\n",
    "            g1 = grad_reg[i].flatten()\n",
    "        else:\n",
    "            g1 = np.insert(g1, len(g1), grad_reg[i].flatten())\n",
    "    \n",
    "    return reg_J,g1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize weights\n",
    "\n",
    "def randInitializeWeights(L_in,L_out):\n",
    "    epi = (2**(1/2))/(L_in+L_out)**(1/2)\n",
    "    # Recall the matrix is like in the other sense and we need to add the bais weight\n",
    "    W = np.random.rand(L_out,L_in+1)*(2*epi)-epi\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates the initial theta according to initial layer size, hidden layer size and number of labels\n",
    "\n",
    "def initialTheta(input_layer_size, hidden_layer_size,num_labels):\n",
    "    theta = []\n",
    "    if type(hidden_layer_size)== int:\n",
    "        theta.append(randInitializeWeights(input_layer_size,hidden_layer_size))\n",
    "        theta.append(randInitializeWeights(hidden_layer_size,num_labels))\n",
    "    else:\n",
    "        k = len(hidden_layer_size)\n",
    "        for i in range(k+1):\n",
    "            if i==0:\n",
    "                theta.append(randInitializeWeights(input_layer_size, hidden_layer_size[0]))\n",
    "            elif i== k:\n",
    "                theta.append(randInitializeWeights(hidden_layer_size[k-1], num_labels))\n",
    "            else:\n",
    "                theta.append(randInitializeWeights(hidden_layer_size[i-1],hidden_layer_size[i]))\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_input: length of each example\n",
    "# n_output: number of classes\n",
    "def randomData(n_examples, n_input, n_output):\n",
    "    X = np.random.rand(n_examples,n_input)\n",
    "    y = np.random.randint(n_output, size=n_examples)+1\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Secant line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def secantCost(X,y,theta,input_layer_size, hidden_layer_size, num_labels, epsilon, Lambda):\n",
    "    J =[]\n",
    "    for i in range(len(theta)):\n",
    "        for j in range(len(theta[i])):\n",
    "            for k in range(len(theta[i][j])):\n",
    "                theta1 = copy.deepcopy(theta)\n",
    "                theta2 = copy.deepcopy(theta)\n",
    "                theta1[i][j][k] = theta[i][j][k]+epsilon\n",
    "                theta2[i][j][k] = theta[i][j][k]-epsilon\n",
    "#                 J1 = computeCost(theta1, X,y.reshape(n_examples,1),input_layer_size, hidden_layer_size, num_labels, Lambda)\n",
    "#                 J2 = computeCost(theta2,X,y.reshape(n_examples,1),input_layer_size, hidden_layer_size, num_labels, Lambda)\n",
    "                J1 = computeCost(flattenTheta(theta1, input_layer_size, hidden_layer_size, num_labels), X,y.reshape(n_examples,1),input_layer_size, hidden_layer_size, num_labels, Lambda)\n",
    "                J2 = computeCost(flattenTheta(theta2, input_layer_size, hidden_layer_size, num_labels),X,y.reshape(n_examples,1),input_layer_size, hidden_layer_size, num_labels, Lambda)\n",
    "                J.append((J1[0]-J2[0])/(2*epsilon))\n",
    "    return J\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.0001\n",
    "Lambda = 1\n",
    "secant= secantCost(X,y.reshape(n_examples,1),initial_theta,input_layer_size, hidden_layer_size, num_labels, epsilon, Lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison between secant line and derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the norm2 of difference between gradient and secant line is smaller than tolerance\n",
    "\n",
    "def checking(grad, sec, tolerance):\n",
    "    # we need flatten grad\n",
    "    \n",
    "    if np.linalg.norm(flattenTheta(grad, input_layer_size, hidden_layer_size, num_labels)-secant) <= tolerance:\n",
    "        return True, np.linalg.norm(flattenTheta(grad, input_layer_size, hidden_layer_size, num_labels)-secant)\n",
    "    else:\n",
    "        return False, np.linalg.norm(flattenTheta(grad, input_layer_size, hidden_layer_size, num_labels)-secant)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize\n",
    "\n",
    "n_examples=3\n",
    "input_layer_size = 2\n",
    "\n",
    "# list of hidden layers'size\n",
    "hidden_layer_size = 5\n",
    "num_labels = 4\n",
    "\n",
    "initial_theta = initialTheta(input_layer_size,hidden_layer_size,num_labels)\n",
    "initial_theta_flatten = flattenTheta(initial_theta, input_layer_size, hidden_layer_size, num_labels)\n",
    "\n",
    "X,y = randomData(n_examples, input_layer_size, num_labels)\n",
    "\n",
    "cost, grad = computeCost(initial_theta_flatten, X, y.reshape(n_examples,1),input_layer_size,hidden_layer_size,num_labels,0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True if the norm 2 to subtract secant line and gradient is smaller than tolerance \n",
    "\n",
    "tolerance = 0.000000001\n",
    "epsilon = 0.0001\n",
    "\n",
    "# should be non regularized\n",
    "Lambda = 0\n",
    "secant= secantCost(X,y.reshape(n_examples,1),initial_theta,input_layer_size, hidden_layer_size, num_labels, epsilon, Lambda)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, 2.806268387653222e-10)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checking(grad, secant,tolerance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
